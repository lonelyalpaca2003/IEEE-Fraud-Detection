{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will concentrate on both removing redundant features by using feature selection methods and possibly creating new features using feature engineering. We will first turn our attention on the issues relating to missing values in our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_pickle('data/df_train_cleaned.pkl')\n",
    "df_test = pd.read_pickle('data/df_test_cleaned.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "112 114 134\n"
     ]
    }
   ],
   "source": [
    "no_missing = []\n",
    "few_missing = []\n",
    "lots_missing = []\n",
    "for col in list(df_train.columns):\n",
    "    prop = round(df_train[col].isna().sum()/len(df_train), 3)\n",
    "    if prop < 0.1:\n",
    "        no_missing.append(col)\n",
    "    if (prop > 0.1) & (prop <= 0.7):\n",
    "        few_missing.append(col)\n",
    "    elif prop > 0.7:\n",
    "        lots_missing.append(col)\n",
    "\n",
    "print(len(no_missing),len(few_missing), len(lots_missing))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's observed that 112 columns have almost no missing values with less than 10% missing values. These observations could possibly be dropped or very simply imputed using available data. Here, I choose to impute the data. However, we see that 114 columns have a proportion of 10-30% missing values and 134 columns have more than 30% missing values. For these columns with a high proportion of missing values, different techniques will have to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in no_missing: \n",
    "    if df_train[col].dtype == 'object':\n",
    "        df_train[col].fillna(df_train[col].mode()[0], inplace = True)\n",
    "    else:\n",
    "        df_train[col].fillna(df_train[col].median(), inplace = True)    \n",
    "\n",
    "for col in few_missing: \n",
    "    if df_train[col].dtype == 'object':\n",
    "        df_train[col].fillna(df_train[col].mode()[0], inplace = True)\n",
    "    else:\n",
    "        df_train[col].fillna(df_train[col].median(), inplace = True)  \n",
    "\n",
    "for col in lots_missing:\n",
    "    if df_train[col].dtype == 'object':\n",
    "        df_train[col].fillna('Unknown', inplace = True)\n",
    "    else:\n",
    "        df_train[col].fillna(-1000, inplace = True)          \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransactionID     0\n",
       "isFraud           0\n",
       "TransactionDT     0\n",
       "TransactionAmt    0\n",
       "ProductCD         0\n",
       "                 ..\n",
       "id_36             0\n",
       "id_37             0\n",
       "id_38             0\n",
       "DeviceType        0\n",
       "DeviceInfo        0\n",
       "Length: 360, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have no missing values in any of the columns, we can move onto feature engineering. However, in order to reduce the sheer size of the number of variables present, I will first implement some feature selection methods such as variance threshold. This will remove those features that contain extremely low predictive power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "numerical_cols = df_test.select_dtypes(include = 'number').columns\n",
    "selector = VarianceThreshold(threshold = threshold)\n",
    "selector.fit(df_train[numerical_cols])\n",
    "columns_to_drop = numerical_cols[~selector.get_support()]\n",
    "df_train = df_train.drop(columns = columns_to_drop)\n",
    "df_test = df_test.drop(columns = columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            count   mean\n",
      "ProductCD               \n",
      "C           68519  0.117\n",
      "H           33024  0.048\n",
      "R           37699  0.038\n",
      "S           11628  0.059\n",
      "W          439670  0.020\n"
     ]
    }
   ],
   "source": [
    "fraud_by_product = df_train.groupby('ProductCD')['isFraud'].agg(['count', 'mean']).round(3)\n",
    "print(fraud_by_product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                   count   mean\n",
      "card6           card4                          \n",
      "charge card     american express       3  0.000\n",
      "                visa                  12  0.000\n",
      "credit          american express    8175  0.029\n",
      "                discover            6304  0.079\n",
      "                mastercard         50772  0.069\n",
      "                visa               83735  0.068\n",
      "debit           american express     150  0.033\n",
      "                discover             347  0.040\n",
      "                mastercard        138415  0.022\n",
      "                visa              302597  0.025\n",
      "debit or credit mastercard            30  0.000\n"
     ]
    }
   ],
   "source": [
    "fraud_by_card = df_train.groupby(['card6', 'card4'])['isFraud'].agg(['count', 'mean']).round(3)\n",
    "print(fraud_by_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>protonmail.com</th>\n",
       "      <td>76</td>\n",
       "      <td>0.408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mail.com</th>\n",
       "      <td>559</td>\n",
       "      <td>0.190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outlook.es</th>\n",
       "      <td>438</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>aim.com</th>\n",
       "      <td>315</td>\n",
       "      <td>0.127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outlook.com</th>\n",
       "      <td>5096</td>\n",
       "      <td>0.095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hotmail.es</th>\n",
       "      <td>305</td>\n",
       "      <td>0.066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>live.com.mx</th>\n",
       "      <td>749</td>\n",
       "      <td>0.055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hotmail.com</th>\n",
       "      <td>45250</td>\n",
       "      <td>0.053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gmail.com</th>\n",
       "      <td>322811</td>\n",
       "      <td>0.039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yahoo.fr</th>\n",
       "      <td>143</td>\n",
       "      <td>0.035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 count   mean\n",
       "P_emaildomain                \n",
       "protonmail.com      76  0.408\n",
       "mail.com           559  0.190\n",
       "outlook.es         438  0.130\n",
       "aim.com            315  0.127\n",
       "outlook.com       5096  0.095\n",
       "hotmail.es         305  0.066\n",
       "live.com.mx        749  0.055\n",
       "hotmail.com      45250  0.053\n",
       "gmail.com       322811  0.039\n",
       "yahoo.fr           143  0.035"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fraud_by_domain = df_train.groupby('P_emaildomain')['isFraud'].agg(['count', 'mean']).round(3)\n",
    "fraud_by_domain.sort_values('mean', ascending = False).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.copy()\n",
    "df_train['hour'] = round((df_train['TransactionDT']/3600)% 24, ndigits = -1)\n",
    "df_train['day_of_week'] = (df_train['TransactionDT'] // (3600 * 24)) % 7\n",
    "df_test['hour'] = round((df_test['TransactionDT']/3600)% 24, ndigits = -1)\n",
    "df_test['day_of_week'] = (df_test['TransactionDT'] // (3600 * 24)) % 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interaction terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['Product_plus_card_type'] = (df_train['ProductCD'] + ' + ' + df_train['card4']).astype(str)\n",
    "df_test['Product_plus_card_type'] = (df_test['ProductCD'] + ' + ' + df_test['card4']).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_pickle('data/df_train_w_feats.pkl')\n",
    "df_test.to_pickle('data/df_test_w_feats.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "st456env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
